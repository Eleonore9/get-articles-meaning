{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting meaning from a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Using TextBlob, NLTK and Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example text:\n",
    "zen = [\"Beautiful is better than ugly. Explicit is better than implicit.\", \n",
    "        \"Simple is better than complex. Complex is better than complicated.\",\n",
    "        \"Flat is better than nested. Sparse is better than dense.\",\n",
    "        \"Readability counts. Special cases aren't special enough to break the rules.\",\n",
    "        \"Although practicality beats purity. Errors should never pass silently.\",\n",
    "        \"Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess.\" ,\n",
    "        \"There should be one-- and preferably only one --obvious way to do it.\",\n",
    "        \"Although that way may not be obvious at first unless you're Dutch.\",\n",
    "        \"Now is better than never. Although never is often better than *right* now.\",\n",
    "        \"If the implementation is hard to explain, it's a bad idea.\",\n",
    "        \"If the implementation is easy to explain, it may be a good idea.\"\n",
    "        \"Namespaces are one honking great idea -- let's do more of those!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', 'counts', 'special', 'cases', 'arent', 'special', 'enough', 'break', 'rules'], ['although', 'practicality', 'beats', 'purity', 'errors', 'should', 'never', 'pass', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', 'lets', 'do', 'more', 'those']]\n"
     ]
    }
   ],
   "source": [
    "## TOKENISATION: Breaking down a text in meaningful elements\n",
    "texts = [text.lower().replace('\\n', ' ').split(' ') for text in zen]\n",
    "\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for'\n",
    "             'from', 'has', 'he', 'if', 'in', 'is', 'it', 'its', 'it\\'s', 'of', 'on', \n",
    "             'than', 'that', 'the', 'to', 'was', 'were', 'will', 'with']\n",
    "\n",
    "docs = [[filter(lambda x:x not in string.punctuation, i) for i in txt if i != '' and i not in stop_words] \n",
    "        for txt in texts]\n",
    "print docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', u'count', 'special', u'case', 'arent', 'special', 'enough', 'break', u'rule'], ['although', 'practicality', u'beat', 'purity', u'error', 'should', 'never', u'pas', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', u'let', 'do', 'more', 'those']]\n"
     ]
    }
   ],
   "source": [
    "## LEMMATISATION:\n",
    "## Grouping together the different forms of a word\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemm = [[lmtzr.lemmatize(word) for word in data] for data in docs]\n",
    "print lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS: Assign a frequency to a word index \n",
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1)], [(1, 2), (5, 2), (6, 1), (7, 1)], [(1, 2), (8, 1), (9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2)], [(20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)], [(29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)], [(27, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1)], [(20, 1), (36, 1), (39, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)], [(1, 2), (20, 1), (23, 2), (50, 2), (51, 1), (52, 1)], [(53, 1), (54, 1), (55, 1), (56, 1), (57, 1)], [(37, 1), (40, 1), (47, 1), (54, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)]]\n",
      "\n",
      "TF-IDF: value associated with the importance of word in a document\n",
      "\n",
      "[(0, 0.46068284809775906), (1, 0.38869686630348355), (2, 0.46068284809775906), (3, 0.46068284809775906), (4, 0.46068284809775906)]\n",
      "[(1, 0.3256764163849216), (5, 0.7719822415102027), (6, 0.38599112075510134), (7, 0.38599112075510134)]\n",
      "[(1, 0.38869686630348355), (8, 0.46068284809775906), (9, 0.46068284809775906), (10, 0.46068284809775906), (11, 0.46068284809775906)]\n",
      "[(12, 0.30151134457776363), (13, 0.30151134457776363), (14, 0.30151134457776363), (15, 0.30151134457776363), (16, 0.30151134457776363), (17, 0.30151134457776363), (18, 0.30151134457776363), (19, 0.6030226891555273)]\n",
      "[(20, 0.20048400621111767), (21, 0.3700038072053449), (22, 0.3700038072053449), (23, 0.2630487209385746), (24, 0.3700038072053449), (25, 0.3700038072053449), (26, 0.3700038072053449), (27, 0.2630487209385746), (28, 0.3700038072053449)]\n",
      "[(29, 0.3650162883572298), (30, 0.3650162883572298), (31, 0.3650162883572298), (32, 0.3650162883572298), (33, 0.3650162883572298), (34, 0.3650162883572298), (35, 0.3650162883572298), (36, 0.2595029183600471)]\n",
      "[(27, 0.2506740298941374), (37, 0.2506740298941374), (38, 0.3525975914172787), (39, 0.2506740298941374), (40, 0.5013480597882748), (41, 0.3525975914172787), (42, 0.3525975914172787), (43, 0.3525975914172787), (44, 0.2506740298941374)]\n",
      "[(20, 0.21561363895750948), (36, 0.2828998333411162), (39, 0.2828998333411162), (44, 0.2828998333411162), (45, 0.3979263423919607), (46, 0.3979263423919607), (47, 0.2828998333411162), (48, 0.3979263423919607), (49, 0.3979263423919607)]\n",
      "[(1, 0.280822745949618), (20, 0.18034197634503563), (23, 0.47324200174991937), (50, 0.6656612575502209), (51, 0.33283062877511044), (52, 0.33283062877511044)]\n",
      "[(53, 0.5332831671781991), (54, 0.3791297610795798), (55, 0.5332831671781991), (56, 0.3791297610795798), (57, 0.3791297610795798)]\n",
      "[(37, 0.204951334419722), (40, 0.204951334419722), (47, 0.204951334419722), (54, 0.204951334419722), (56, 0.204951334419722), (57, 0.204951334419722), (58, 0.2882841389858761), (59, 0.2882841389858761), (60, 0.2882841389858761), (61, 0.2882841389858761), (62, 0.2882841389858761), (63, 0.2882841389858761), (64, 0.2882841389858761), (65, 0.2882841389858761), (66, 0.2882841389858761)]\n"
     ]
    }
   ],
   "source": [
    "## Create bag of words from dictionnary:\n",
    "####note: compare doc2bow and word2vec\n",
    "dictionary = Dictionary(lemm)\n",
    "dictionary.save('text.dict')\n",
    "\n",
    "## Term frequencyâ€“inverse document frequency (TF-IDF)\n",
    "## Method to reflect how important a word is to a document in a collection.\n",
    "## The inverse document frequency measures whether the term is common or rare across all documents.\n",
    "\n",
    "bow = [dictionary.doc2bow(l) for l in lemm] # Calculates inverse document counts for all terms\n",
    "print \"BAG OF WORDS: Assign a frequency to a word index \\n\", bow\n",
    "\n",
    "# Transform the count representation into the Tfidf space\n",
    "tfidf = models.TfidfModel(bow)              \n",
    "corpus_tfidf = tfidf[bow]\n",
    "print \"\\nTF-IDF: value associated with the importance of word in a document\\n\"\n",
    "for doc in corpus_tfidf:\n",
    "    print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.54498357184461965), (1, -0.2126075637752336), (2, 0.20331906793399038)]\n",
      "[(0, 0.49872076945560406), (1, -0.19698495321056153), (2, 0.200134661406946)]\n",
      "[(0, 0.54498357184462021), (1, -0.21260756377523396), (2, 0.20331906793399096)]\n",
      "[]\n",
      "[(0, 0.29788909810577829), (1, 0.11063707308686081), (2, -0.49983116799619542)]\n",
      "[(0, 0.037992769637636571), (1, 0.088009881979444121), (2, -0.21744299165958814)]\n",
      "[(0, 0.19434230433693411), (1, 0.52719369649662684), (2, -0.29033187162354179)]\n",
      "[(0, 0.19783074635880496), (1, 0.38294184000348508), (2, -0.44863074249669871)]\n",
      "[(0, 0.57585999369002672), (1, -0.09944815362681389), (2, -0.23138923147744478)]\n",
      "[(0, 0.10520154691688784), (1, 0.50564529531779667), (2, 0.52535252894323625)]\n",
      "[(0, 0.17251625291638517), (1, 0.69288699106334739), (2, 0.34135753883295711)]\n"
     ]
    }
   ],
   "source": [
    "## Build the LSI (latent semantic indexing) model\n",
    "## Method to uses a mathematical technique called singular value decomposition (SVD) \n",
    "## to identify patterns in the relationships between the terms and concepts contained \n",
    "## in an unstructured collection of text.\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "\n",
    "for doc in corpus_lsi:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5409971672782129, u'better'), (0.27853037146820514, u'complex'), (0.27731795093733874, u'now'), (0.2538440596703842, u'never'), (0.18163236900752422, u'beautiful'), (0.18163236900752411, u'dense'), (0.18163236900752402, u'flat'), (0.18163236900752402, u'nested'), (0.18163236900752394, u'ugly'), (0.18163236900752391, u'sparse')]\n",
      "[(0.30794775806820196, u'one'), (0.25292223047307438, u'explain'), (0.25292223047307438, u'implementation'), (0.25292223047307438, u'idea'), (0.20778805050884408, u'do'), (0.20437009294542946, u'bad'), (0.2043700929454294, u'hard'), (-0.19505419477988611, u'better'), (0.18973512044056642, u'may'), (0.18226648505043833, u'way')]\n",
      "[(0.24330841921959795, u'bad'), (0.24330841921959795, u'hard'), (0.23373524267885862, u'implementation'), (0.23373524267885862, u'explain'), (0.23373524267885862, u'idea'), (-0.20928347389567462, u'never'), (-0.20727316973039475, u'although'), (-0.17738986606596757, u'should'), (-0.17342762797286926, u'way'), (-0.17342762797286926, u'obvious')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(lsi.num_topics):\n",
    "    print lsi.show_topic(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'better', u'one', u'complex', u'now', u'never', u'explain', u'implementation', u'idea', u'bad', u'hard', u'implementation', u'explain', u'idea', u'do', u'bad', u'hard', u'may', u'way', u'beautiful', u'dense', u'flat', u'nested', u'ugly', u'sparse', u'way', u'obvious', u'should', u'better', u'although', u'never']\n"
     ]
    }
   ],
   "source": [
    "list_topics = [] \n",
    "for i in range(lsi.num_topics):\n",
    "    list_topics.extend(lsi.show_topic(i))\n",
    "\n",
    "list_topics.sort(key=lambda tup: tup[0], reverse=True)\n",
    "\n",
    "topics = [i[1] for i in list_topics]\n",
    "print topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
