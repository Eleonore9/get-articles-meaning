{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Getting meaning from scientific articles\n",
    "\n",
    "### Eleonore Mayola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Who I am\n",
    "\n",
    "* Background in biomedical research\n",
    "\n",
    "* Learned Python\n",
    "\n",
    "* Magically turned into a dev -> now \"data scientist\"\n",
    "\n",
    "=> What would I have do had I know to code back in my lab days?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####What's my idea\n",
    "\n",
    "Q: Scientists have to regularly scan through the recent publications in their area.\n",
    "   \n",
    "   __How to make the bibliography process faster and less boring?__\n",
    "\n",
    "\n",
    "idea: Getting topics info from scientific articles depending on the sub-area\n",
    "\n",
    "Being able to classify a new article in one of those sub-areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "## Example text:\n",
    "zen = [\"Beautiful is better than ugly. Explicit is better than implicit.\", \n",
    "        \"Simple is better than complex. Complex is better than complicated.\",\n",
    "        \"Flat is better than nested. Sparse is better than dense.\",\n",
    "        \"Readability counts. Special cases aren't special enough to break the rules.\",\n",
    "        \"Although practicality beats purity. Errors should never pass silently.\",\n",
    "        \"Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess.\" ,\n",
    "        \"There should be one-- and preferably only one --obvious way to do it.\",\n",
    "        \"Although that way may not be obvious at first unless you're Dutch.\",\n",
    "        \"Now is better than never. Although never is often better than *right* now.\",\n",
    "        \"If the implementation is hard to explain, it's a bad idea.\",\n",
    "        \"If the implementation is easy to explain, it may be a good idea.\"\n",
    "        \"Namespaces are one honking great idea -- let's do more of those!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 172 articles in articles/Neuroscience/\n",
      "There are 67 articles in articles/Cell biology/\n",
      "[u'scale', u'colocalization', u'vglut1', u'pearsons', u'coefficient', u'panel', u'red', u'colocalization', u'entire', u'hippocampal']\n",
      "\n",
      "\n",
      "This script took 2.222058 seconds to run\n"
     ]
    }
   ],
   "source": [
    "# %load get_topics.py\n",
    "#!/home/eleonore/virtenvs/nltk-gensim-skl/bin/python2.7\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string, re, codecs, time\n",
    "\n",
    "\n",
    "## Global variables\n",
    "stop_words = ['a', 'also', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'but', 'for',\n",
    "              'from', 'has', 'he', 'if', 'in', 'is', 'it', 'its', 'it\\'s', 'not',\n",
    "              'of', 'on', 'our', 'than', 'that', 'the', 'therefore', 'to', 'was',\n",
    "              'were', 'will', 'with', 'may', 'need', 'have', 'been', 'their', 'this',\n",
    "              'these', 'which', 'do', 'did', 'red', 'blue', 'green', 'bar', 'chart',\n",
    "              'arrowhead', 'arrow', 'vice', 'versa']\n",
    "\n",
    "spe_char = {u'β': 'beta', u'α': 'alpha', u'µm': 'micron'}\n",
    "\n",
    "## Functions to break up the process:\n",
    "def parse_text(text_file):\n",
    "    \"Gets a text file outputs a list of strings.\"\n",
    "    with codecs.open(text_file, mode='r', encoding='utf-8') as f:\n",
    "        read = f.read()\n",
    "        r = [read.replace(unicode(i), spe_char.get(i)) for i in read if i in spe_char.keys()] or [read]\n",
    "        text = [line for line in r[0].strip().split('. ') if line != '']\n",
    "        return text\n",
    "    \n",
    "def get_tokens(text_parsed):\n",
    "    \"Gets a text and retrieves tokens.\"\n",
    "    # Tokenisation\n",
    "    texts = [t.lower().replace('\\n', ' ').split(' ') for t in text_parsed]\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [[filter(lambda x:x not in string.punctuation, i)\n",
    "               for i in txt if i != '' and i not in stop_words] for txt in texts]\n",
    "    #print len(tokens), [len(txt) for txt in tokens]\n",
    "    tokens_cleaned = [[i for i in txt if len(i) > 2 and not i.isdigit()] for txt in tokens]\n",
    "    #print len(tokens_cleaned), [len(txt) for txt in tokens_cleaned]\n",
    "    return tokens_cleaned\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"Gets tokens and retrieves lemmatised tokens.\"\n",
    "    # Lemmatisation using nltk lemmatiser\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma = [[lmtzr.lemmatize(word) for word in data] for data in tokens]\n",
    "    return lemma\n",
    "\n",
    "def bag_of_words(lemma):\n",
    "    \"Takes in lemmatised words and returns a bow.\"\n",
    "    ## Create bag of words from dictionnary\n",
    "    dictionary = Dictionary(lemma)\n",
    "    dictionary.save('text.dict')\n",
    "    ## Term frequency–inverse document frequency (TF-IDF)\n",
    "    bow = [dictionary.doc2bow(l) for l in lemma] # Calculates inverse document counts for all terms\n",
    "    return bow\n",
    "\n",
    "def tfidf_and_lsi(lemma, bow):\n",
    "    \"Gets a bow and returns topics.\"\n",
    "    dictionary = Dictionary(lemma)\n",
    "    tfidf = models.TfidfModel(bow) # Transforms the count representation into the Tfidf space\n",
    "    corpus_tfidf = tfidf[bow]\n",
    "    ## Build the LSI model\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    list_topics = []\n",
    "    for i in range(lsi.num_topics):\n",
    "        list_topics.extend(lsi.show_topic(i))\n",
    "    list_topics.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    # for i in range(lsi.num_topics):\n",
    "    #     list_topics.extend(lsi.show_topic(i))\n",
    "    topics = [i[1] for i in list_topics[:10]]\n",
    "    return topics\n",
    "\n",
    "## Function to retrieve topics using nltk\n",
    "def get_topics(text_file):\n",
    "    txt = parse_text(text_file)\n",
    "    tokens = get_tokens(txt)\n",
    "    #print tokens\n",
    "    lemma = lemmatize_tokens(tokens)\n",
    "    bow = bag_of_words(lemma)\n",
    "    return tfidf_and_lsi(lemma, bow)\n",
    "\n",
    "## Get all text articles from a path and retrieve topics:\n",
    "def list_all_articles(path):\n",
    "    articles = [f for f in listdir(path) if isfile(join(path, f))] or []\n",
    "    print \"There are %d articles in %s\" % (len(articles), path)\n",
    "    return {\"path\": path, \"articles\": articles}\n",
    "\n",
    "def get_articles_topics(path, filename):\n",
    "    \"Store the topics in a json object and dump to a file.\"\n",
    "    #Get the directories in a path\n",
    "    #For each dir and for each file in a dir\n",
    "    #{\"Neuroscience\": [[topics_file1], [topics_file2]...]\n",
    "    # \"Cell biology\": [[], []...]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    startTime = time.time()\n",
    "\n",
    "    #print get_topics('articles/conrad2013_melanoma.txt')\n",
    "\n",
    "    neuro_articles = list_all_articles(\"articles/Neuroscience/\")\n",
    "    cellbiol_articles = list_all_articles(\"articles/Cell biology/\")\n",
    "    #print get_tokens(parse_text(neuro_articles.get(\"path\") + neuro_articles.get(\"articles\")[0]))\n",
    "    print get_topics(neuro_articles.get(\"path\") + neuro_articles.get(\"articles\")[0])\n",
    "\n",
    "    print \"\\n\"\n",
    "    elapsedTime = time.time() - startTime\n",
    "    print \"This script took %f seconds to run\" % elapsedTime\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'per',\n",
       " u'padn\\u2212\\u2212',\n",
       " u'padn',\n",
       " u'condition',\n",
       " u'padn\\u2212\\u2212',\n",
       " u'padn',\n",
       " u'mouse',\n",
       " u'glucose',\n",
       " u'blood',\n",
       " u'fasting']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of running my script to get topics from an article\n",
    "\n",
    "import get_topics as gt\n",
    "\n",
    "gt.get_topics(\"articles/Cell biology/elife00013.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
