{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PyData London 2015\n",
    "\n",
    "\n",
    "#Getting meaning from scientific articles\n",
    "\n",
    "\n",
    "## Eleonore Mayola, PhD\n",
    "####Data Scientist at Mastodonc\n",
    "####Co-organiser at PyLadies London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h6><b>PyData London 2015</b> - <em>@EleonoreMayola</em></h6>\n",
    "\n",
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- What's this about\n",
    "\n",
    "3- How I planned for this project\n",
    "\n",
    "4- How I executed my plan\n",
    "\n",
    "5- What went wrong\n",
    "\n",
    "6 - What's next\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "__1- Who I am__\n",
    "\n",
    "2- What's this about\n",
    "\n",
    "3- How I planned for this project\n",
    "\n",
    "4- How I executed my plan\n",
    "\n",
    "5- What went wrong\n",
    "\n",
    "6 - What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Who I am\n",
    "\n",
    "* Background in biomedical research -> no programming experience\n",
    "\n",
    "* Learned Python at an intensive software eng course -> career change\n",
    "\n",
    "* Magically turned into a developer -> full-stack web dev + Python scripting\n",
    "\n",
    "* Now \"data scientist\" -> science / research + software / web development\n",
    "\n",
    "\n",
    "=> What would I have done had I known how to code back in my lab days?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "__2- What's this about__\n",
    "\n",
    "3- How I planned for this project\n",
    "\n",
    "4- How I executed my plan\n",
    "\n",
    "5- What went wrong\n",
    "\n",
    "6 - What's next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####What's this about\n",
    "\n",
    "*Problem*: Scientists have to regularly scan through the recent publications in their area.\n",
    "   \n",
    "   __How to make the bibliography process less time-consuming?__\n",
    "\n",
    "\n",
    "\n",
    "*Idea*: Gather info from scientific articles of different the subcategories (I focused on Biology).\n",
    "\n",
    "__Be able to classify a new article in one of those sub-categories__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- What's this about\n",
    "\n",
    "__3- How I planned for this project__\n",
    "\n",
    "4- How I executed my plan\n",
    "\n",
    "5- What went wrong\n",
    "\n",
    "6 - What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I planned for this project\n",
    "\n",
    "     \n",
    "     \n",
    "                        NLTK                                        \n",
    "  Articles subject A    ---->   most frequent terms (A)  --------\n",
    "                                                                -\n",
    "                                                                  SciKit-Learn\n",
    "                                                                  ------------>  Classifier to predict if an\n",
    "                                                                                 article is of subcategory A or B\n",
    "                        NLTK                                    -\n",
    "  Articles subject B    ---->   most frequent terms (B)  --------\n",
    "                                          \n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- What's this about\n",
    "\n",
    "3- How I planned for this project\n",
    "\n",
    "__4- How I executed my plan__\n",
    "\n",
    "5- What went wrong\n",
    "\n",
    "6 - What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 1: Gathering scientific articles\n",
    "\n",
    "\n",
    "                        NLTK                                        \n",
    "  Articles subject A    ---->   most frequent terms (A)  --------\n",
    "                                                                -\n",
    "                                                                  SciKit-Learn\n",
    "                                                                  ------------>  Classifier to predict if an\n",
    "                                                                                 article is of subcategory A or B\n",
    "                        NLTK                                    -\n",
    "  Articles subject B    ---->   most frequent terms (B)  --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 1: Gathering scientific articles\n",
    "\n",
    "1 - Yay for Open Access research articles!\n",
    "\n",
    "2 - Hmmm, where's the API?\n",
    "\n",
    "3 - My solution: eLifeScience articles on Github (http://github.com/elifesciences/elife-articles)\n",
    "\n",
    "4 - Parsing XML files using Beautiful Soup -> Group A: \"Neuroscience\" - Group B: \"Cell biology\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 2: Retrieve the most frequent terms\n",
    "\n",
    "\n",
    "                        NLTK                                        \n",
    "  Articles subject A    ---->   most frequent terms (A)  --------\n",
    "                                                                -\n",
    "                                                                  SciKit-Learn\n",
    "                                                                  ------------>  Classifier to predict if an\n",
    "                                                                                 article is of subcategory A or B\n",
    "                        NLTK                                    -\n",
    "  Articles subject B    ---->   most frequent terms (B)  --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 2: Retrieve the most frequent terms\n",
    " \n",
    "-> Use Nltk and Gensim libraries to implement the TF-IDF (Term frequencyâ€“inverse document frequency) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "## Example text:\n",
    "zen = [\"Beautiful is better than ugly. Explicit is better than implicit.\", \n",
    "        \"Simple is better than complex. Complex is better than complicated.\",\n",
    "        \"Flat is better than nested. Sparse is better than dense.\",\n",
    "        \"Readability counts. Special cases aren't special enough to break the rules.\",\n",
    "        \"Although practicality beats purity. Errors should never pass silently.\",\n",
    "        \"Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess.\" ,\n",
    "        \"There should be one-- and preferably only one --obvious way to do it.\",\n",
    "        \"Although that way may not be obvious at first unless you're Dutch.\",\n",
    "        \"Now is better than never. Although never is often better than *right* now.\",\n",
    "        \"If the implementation is hard to explain, it's a bad idea.\",\n",
    "        \"If the implementation is easy to explain, it may be a good idea.\"\n",
    "        \"Namespaces are one honking great idea -- let's do more of those!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', 'counts', 'special', 'cases', 'arent', 'special', 'enough', 'break', 'rules'], ['although', 'practicality', 'beats', 'purity', 'errors', 'should', 'never', 'pass', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', 'lets', 'do', 'more', 'those']]\n"
     ]
    }
   ],
   "source": [
    "## TOKENISATION: Breaking down a text in meaningful elements\n",
    "texts = [text.lower().replace('\\n', ' ').split(' ') for text in zen]\n",
    "\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for'\n",
    "             'from', 'has', 'he', 'if', 'in', 'is', 'it', 'its', 'it\\'s', 'of', 'on', \n",
    "             'than', 'that', 'the', 'to', 'was', 'were', 'will', 'with']\n",
    "\n",
    "docs = [[filter(lambda x:x not in string.punctuation, i) for i in txt if i != '' and i not in stop_words] \n",
    "        for txt in texts]\n",
    "print docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', u'count', 'special', u'case', 'arent', 'special', 'enough', 'break', u'rule'], ['although', 'practicality', u'beat', 'purity', u'error', 'should', 'never', u'pas', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', u'let', 'do', 'more', 'those']]\n"
     ]
    }
   ],
   "source": [
    "## LEMMATISATION: Grouping together the different forms of a word\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemm = [[lmtzr.lemmatize(word) for word in data] for data in docs]\n",
    "print lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS: Assign a frequency to a word index \n",
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1)], [(1, 2), (5, 2), (6, 1), (7, 1)], [(1, 2), (8, 1), (9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2)], [(20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)], [(29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)], [(27, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1)], [(20, 1), (36, 1), (39, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)], [(1, 2), (20, 1), (23, 2), (50, 2), (51, 1), (52, 1)], [(53, 1), (54, 1), (55, 1), (56, 1), (57, 1)], [(37, 1), (40, 1), (47, 1), (54, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)]]\n"
     ]
    }
   ],
   "source": [
    "## Create bag of words from dictionnary:\n",
    "dictionary = Dictionary(lemm)\n",
    "dictionary.save('text.dict')\n",
    "\n",
    "## Term frequencyâ€“inverse document frequency (TF-IDF)\n",
    "## Method to reflect how important a word is to a document in a collection of documents.\n",
    "## The inverse document frequency measures whether the term is common or rare across all documents.\n",
    "\n",
    "bow = [dictionary.doc2bow(l) for l in lemm] # Calculates inverse document counts for all terms\n",
    "print \"BAG OF WORDS: Assign a frequency to a word index \\n\", bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF: value associated with the importance of word in a document\n",
      "\n",
      "[(0, 0.46068284809775906), (1, 0.38869686630348355), (2, 0.46068284809775906), (3, 0.46068284809775906), (4, 0.46068284809775906)]\n",
      "[(1, 0.3256764163849216), (5, 0.7719822415102027), (6, 0.38599112075510134), (7, 0.38599112075510134)]\n",
      "[(1, 0.38869686630348355), (8, 0.46068284809775906), (9, 0.46068284809775906), (10, 0.46068284809775906), (11, 0.46068284809775906)]\n",
      "[(12, 0.30151134457776363), (13, 0.30151134457776363), (14, 0.30151134457776363), (15, 0.30151134457776363), (16, 0.30151134457776363), (17, 0.30151134457776363), (18, 0.30151134457776363), (19, 0.6030226891555273)]\n",
      "[(20, 0.20048400621111767), (21, 0.3700038072053449), (22, 0.3700038072053449), (23, 0.2630487209385746), (24, 0.3700038072053449), (25, 0.3700038072053449), (26, 0.3700038072053449), (27, 0.2630487209385746), (28, 0.3700038072053449)]\n",
      "[(29, 0.3650162883572298), (30, 0.3650162883572298), (31, 0.3650162883572298), (32, 0.3650162883572298), (33, 0.3650162883572298), (34, 0.3650162883572298), (35, 0.3650162883572298), (36, 0.2595029183600471)]\n",
      "[(27, 0.2506740298941374), (37, 0.2506740298941374), (38, 0.3525975914172787), (39, 0.2506740298941374), (40, 0.5013480597882748), (41, 0.3525975914172787), (42, 0.3525975914172787), (43, 0.3525975914172787), (44, 0.2506740298941374)]\n",
      "[(20, 0.21561363895750948), (36, 0.2828998333411162), (39, 0.2828998333411162), (44, 0.2828998333411162), (45, 0.3979263423919607), (46, 0.3979263423919607), (47, 0.2828998333411162), (48, 0.3979263423919607), (49, 0.3979263423919607)]\n",
      "[(1, 0.280822745949618), (20, 0.18034197634503563), (23, 0.47324200174991937), (50, 0.6656612575502209), (51, 0.33283062877511044), (52, 0.33283062877511044)]\n",
      "[(53, 0.5332831671781991), (54, 0.3791297610795798), (55, 0.5332831671781991), (56, 0.3791297610795798), (57, 0.3791297610795798)]\n",
      "[(37, 0.204951334419722), (40, 0.204951334419722), (47, 0.204951334419722), (54, 0.204951334419722), (56, 0.204951334419722), (57, 0.204951334419722), (58, 0.2882841389858761), (59, 0.2882841389858761), (60, 0.2882841389858761), (61, 0.2882841389858761), (62, 0.2882841389858761), (63, 0.2882841389858761), (64, 0.2882841389858761), (65, 0.2882841389858761), (66, 0.2882841389858761)]\n"
     ]
    }
   ],
   "source": [
    "# Transform the count representation into the Tfidf space\n",
    "tfidf = models.TfidfModel(bow)              \n",
    "corpus_tfidf = tfidf[bow]\n",
    "print \"\\nTF-IDF: value associated with the importance of word in a document\\n\"\n",
    "for doc in corpus_tfidf:\n",
    "    print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -0.54498357184461865), (1, 0.21260756377523921), (2, -0.2033190679339878)]\n",
      "[(0, -0.49872076945560256), (1, 0.1969849532105663), (2, -0.20013466140694447)]\n",
      "[(0, -0.54498357184461899), (1, 0.21260756377523851), (2, -0.2033190679339881)]\n",
      "[]\n",
      "[(0, -0.29788909810577979), (1, -0.11063707308686005), (2, 0.49983116799619021)]\n",
      "[(0, -0.037992769637637043), (1, -0.088009881979444468), (2, 0.21744299165959061)]\n",
      "[(0, -0.19434230433693714), (1, -0.52719369649662484), (2, 0.29033187162354313)]\n",
      "[(0, -0.19783074635880774), (1, -0.38294184000348114), (2, 0.44863074249670343)]\n",
      "[(0, -0.57585999369002594), (1, 0.099448153626818428), (2, 0.2313892314774415)]\n",
      "[(0, -0.10520154691689336), (1, -0.50564529531779601), (2, -0.52535252894323925)]\n",
      "[(0, -0.17251625291639155), (1, -0.69288699106334639), (2, -0.34135753883295689)]\n"
     ]
    }
   ],
   "source": [
    "## Build the LSI (latent semantic indexing) model\n",
    "## Method to uses a mathematical technique called singular value decomposition (SVD) \n",
    "## to identify patterns in the relationships between the terms and concepts contained \n",
    "## in an unstructured collection of text.\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "\n",
    "for doc in corpus_lsi:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.54099716727821134, u'better'), (-0.27853037146820414, u'complex'), (-0.2773179509373383, u'now'), (-0.25384405967038426, u'never'), (-0.18163236900752383, u'nested'), (-0.18163236900752361, u'sparse'), (-0.18163236900752361, u'dense'), (-0.18163236900752361, u'beautiful'), (-0.18163236900752355, u'flat'), (-0.18163236900752355, u'implicit')]\n",
      "[(-0.30794775806820107, u'one'), (-0.25292223047307416, u'explain'), (-0.25292223047307416, u'implementation'), (-0.25292223047307416, u'idea'), (-0.20778805050884358, u'do'), (-0.20437009294542927, u'bad'), (-0.20437009294542927, u'hard'), (0.19505419477989078, u'better'), (-0.18973512044056556, u'may'), (-0.18226648505043708, u'way')]\n",
      "[(-0.24330841921959975, u'bad'), (-0.24330841921959975, u'hard'), (-0.23373524267885962, u'implementation'), (-0.23373524267885962, u'idea'), (-0.23373524267885962, u'explain'), (0.20928347389567176, u'never'), (0.20727316973039422, u'although'), (0.17738986606596632, u'should'), (0.1734276279728709, u'obvious'), (0.1734276279728709, u'way')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(lsi.num_topics):\n",
    "    print lsi.show_topic(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'never', u'although', u'better', u'should', u'obvious', u'way', u'flat', u'implicit', u'sparse', u'dense', u'beautiful', u'nested', u'way', u'may', u'bad', u'hard', u'do', u'implementation', u'idea', u'explain', u'bad', u'hard', u'explain', u'implementation', u'idea', u'never', u'now', u'complex', u'one', u'better']\n"
     ]
    }
   ],
   "source": [
    "list_topics = [] \n",
    "for i in range(lsi.num_topics):\n",
    "    list_topics.extend(lsi.show_topic(i))\n",
    "\n",
    "list_topics.sort(key=lambda tup: tup[0], reverse=True)\n",
    "\n",
    "topics = [i[1] for i in list_topics]\n",
    "print topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 2: Retrieve the most frequent terms\n",
    " \n",
    "-> Convert the script to functions to be executed on a collection of text files (converted from XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 172 articles in articles/Neuroscience/\n",
      "There are 67 articles in articles/Cell biology/\n",
      "[u'vglut1', u'arrowhead', u'domain', u'dcvs', u'caps1', u'overlapping', u'arrowhead', u'vglut1', u'domain', u'secretion']\n"
     ]
    }
   ],
   "source": [
    "# %load get_topics.py\n",
    "\n",
    "#!/home/eleonore/virtenvs/nltk-gensim-skl/bin/python2.7\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string, re, codecs, time, json\n",
    "\n",
    "\n",
    "## Global variables\n",
    "stop_words = ['a', 'also', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'but', 'for',\n",
    "              'from', 'has', 'he', 'if', 'in', 'is', 'it', 'its', 'it\\'s', 'not',\n",
    "              'of', 'on', 'our', 'than', 'that', 'the', 'therefore', 'to', 'was',\n",
    "              'were', 'will', 'with', 'may', 'need', 'have', 'been', 'their', 'this',\n",
    "              'these', 'which', 'do', 'did', 'red', 'blue', 'green', 'bar', 'chart',\n",
    "              'arrowhead', 'arrow', 'vice', 'versa']\n",
    "\n",
    "spe_char = {u'Î²': 'beta', u'Î±': 'alpha', u'Âµm': 'micron'}\n",
    "\n",
    "## Functions to break up the process:\n",
    "def parse_text(text_file):\n",
    "    \"Gets a text file outputs a list of strings.\"\n",
    "    with codecs.open(text_file, mode='r', encoding='utf-8') as f:\n",
    "        read = f.read()\n",
    "        r = [read.replace(unicode(i), spe_char.get(i)) for i in read if i in spe_char.keys()] or [read]\n",
    "        text = [line for line in r[0].strip().split('. ') if line != '']\n",
    "        return text\n",
    "    \n",
    "def get_tokens(text_parsed):\n",
    "    \"Gets a text and retrieves tokens.\"\n",
    "    # Tokenisation\n",
    "    texts = [t.lower().replace('\\n', ' ').split(' ') for t in text_parsed]\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [[filter(lambda x:x not in string.punctuation, i)\n",
    "               for i in txt if i != '' and i not in stop_words] for txt in texts]\n",
    "    #print len(tokens), [len(txt) for txt in tokens]\n",
    "    tokens_cleaned = [[i for i in txt if len(i) > 2 and not i.isdigit()] for txt in tokens]\n",
    "    #print len(tokens_cleaned), [len(txt) for txt in tokens_cleaned]\n",
    "    return tokens_cleaned\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"Gets tokens and retrieves lemmatised tokens.\"\n",
    "    # Lemmatisation using nltk lemmatiser\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma = [[lmtzr.lemmatize(word) for word in data] for data in tokens]\n",
    "    return lemma\n",
    "\n",
    "def bag_of_words(lemma):\n",
    "    \"Takes in lemmatised words and returns a bow.\"\n",
    "    ## Create bag of words from dictionnary\n",
    "    dictionary = Dictionary(lemma)\n",
    "    dictionary.save('text.dict')\n",
    "    ## Term frequencyâ€“inverse document frequency (TF-IDF)\n",
    "    bow = [dictionary.doc2bow(l) for l in lemma] # Calculates inverse document counts for all terms\n",
    "    return bow\n",
    "\n",
    "def tfidf_and_lsi(lemma, bow):\n",
    "    \"Gets a bow and returns topics.\"\n",
    "    dictionary = Dictionary(lemma)\n",
    "    tfidf = models.TfidfModel(bow) # Transforms the count representation into the Tfidf space\n",
    "    corpus_tfidf = tfidf[bow]\n",
    "    ## Build the LSI model\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    list_topics = []\n",
    "    for i in range(lsi.num_topics):\n",
    "        list_topics.extend(lsi.show_topic(i))\n",
    "    list_topics.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    # for i in range(lsi.num_topics):\n",
    "    #     list_topics.extend(lsi.show_topic(i))\n",
    "    topics = [i[1] for i in list_topics[:10]]\n",
    "    return topics\n",
    "\n",
    "## Function to retrieve topics using nltk\n",
    "def get_topics(text_file):\n",
    "    txt = parse_text(text_file)\n",
    "    tokens = get_tokens(txt)\n",
    "    #print tokens\n",
    "    lemma = lemmatize_tokens(tokens)\n",
    "    bow = bag_of_words(lemma)\n",
    "    return tfidf_and_lsi(lemma, bow)\n",
    "\n",
    "## Get all text articles from a path and retrieve topics:\n",
    "def list_all_articles(path):\n",
    "    articles = [f for f in listdir(path) if isfile(join(path, f))] or []\n",
    "    print \"There are %d articles in %s\" % (len(articles), path)\n",
    "    return {\"path\": path, \"articles\": articles}\n",
    "\n",
    "# Write the topics to a json file:\n",
    "#{\"Neuroscience\": {\"pub_id1\":[topics_file1], \"pub_id2\":[topics_file2]...},\n",
    "# \"Cell biology\": {[], []...}}\n",
    "def get_articles_topics(path, filename):\n",
    "    \"Store the topics in a json object and dump to a file.\"\n",
    "    all_topics = {}\n",
    "    #Get the directories in a path\n",
    "    dirs = [d for d in listdir(path) if not isfile(join(path, d))]\n",
    "    #For each dir and for each file in a dir\n",
    "    for d in dirs:\n",
    "        all_topics[d] = {}\n",
    "        txt_files = [f for f in listdir(path+d) if isfile(join(path+d, f))]\n",
    "        print txt_files\n",
    "        for f in txt_files:\n",
    "            all_topics[d][f[:-4]] = get_topics(path+d+'/'+f)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_topics, f)\n",
    "    return all_topics\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    neuro_articles = list_all_articles(\"articles/Neuroscience/\")\n",
    "    \n",
    "    cellbiol_articles = list_all_articles(\"articles/Cell biology/\")\n",
    "    \n",
    "    print get_topics(neuro_articles.get(\"path\") + neuro_articles.get(\"articles\")[0])\n",
    "\n",
    "    #print get_articles_topics(\"articles/\", \"filenames_topics.json\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 3: Build a classifier to predict the subcategory of an article\n",
    "\n",
    "\n",
    "                        NLTK                                        \n",
    "  Articles subject A    ---->   most frequent terms (A)  --------\n",
    "                                                                -\n",
    "                                                                  SciKit-Learn\n",
    "                                                                  ------------>  Classifier to predict if an\n",
    "                                                                                 article is of subcategory A or B\n",
    "                        NLTK                                    -\n",
    "  Articles subject B    ---->   most frequent terms (B)  --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 1: Exploring the data with Numpy and Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "with open(\"filenames_topics.json\", 'r') as f:\n",
    "        topics_articles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'scale', u'detected', u'inset', u'could', u'scale', u'rnai', u'antiphosphoret', u'development', u'control', u'receptor']\n",
      "[u'allele', u'marker', u'vastus', u'lateralis', u'staining', u'ryr1ag', u'old', u'arrow', u'1year', u'background']\n"
     ]
    }
   ],
   "source": [
    "print topics_articles['Neuroscience']['elife05116']\n",
    "\n",
    "print topics_articles['Cell biology']['elife02923']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
