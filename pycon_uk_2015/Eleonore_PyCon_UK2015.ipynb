{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PyCon UK 2015 - Science track\n",
    "\n",
    "<br/>\n",
    "\n",
    "##Getting meaning from scientific articles\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<span><h3> Eleonore Mayola, PhD</h3>\n",
    "<h4>Data wrangler at Mastodonc</h4>\n",
    "<h4>Organiser at PyLadies London</h4></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- Goal of this project\n",
    "\n",
    "3- My plan\n",
    "\n",
    "4- First iteration\n",
    "\n",
    "5- Improvements\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "7- Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "__1- Who I am__\n",
    "\n",
    "2- Goal of this project\n",
    "\n",
    "3- My plan\n",
    "\n",
    "4- First iteration\n",
    "\n",
    "5- Improvements\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "7- Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Who I am\n",
    "\n",
    "* Background in biomedical research \n",
    "\n",
    "\n",
    "* Learned Python at an intensive course \n",
    "\n",
    "\n",
    "* Now developer, data wrangler, hardware tinkerer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "=> What would I have done had I known how to code back in my lab days?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "__2- Goal of this project__\n",
    "\n",
    "3- My plan\n",
    "\n",
    "4- First iteration\n",
    "\n",
    "5- Improvements\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "7- Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Goal of the project\n",
    "\n",
    "*Problem*: Bibliography is an important activity that takes time off research work.\n",
    "![Plan chart](img/phdComics1_.png)\n",
    "   \n",
    " -> Could the bibliography process be less time-consuming?\n",
    "\n",
    "\n",
    "\n",
    "*Idea*: What if it was possible to classify a new article in one of those sub-categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- Goal of this project\n",
    "\n",
    "__3- My plan__\n",
    "\n",
    "4- First iteration\n",
    "\n",
    "5- Improvements\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "7- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####My plan\n",
    "\n",
    "\n",
    "![Plan chart](img/precursor-doc-0.png)                                         \n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- Goal of this project\n",
    "\n",
    "3- My plan\n",
    "\n",
    "__4- First iteration__\n",
    "\n",
    "5- Improvements\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "7- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####First iteration\n",
    "\n",
    "\n",
    "STEP 1: Gathering scientific articles\n",
    "\n",
    "\n",
    "![Plan chart](img/precursor-doc-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####First iteration\n",
    "\n",
    "\n",
    "STEP 1: Gathering scientific articles\n",
    "\n",
    "1 - Yay for Open Access research articles!\n",
    "\n",
    "2 - Hmmm, where's the API so I can upload hundreds of them?\n",
    "\n",
    "3 - My solution: eLifeScience articles on Github (http://github.com/elifesciences/elife-articles)\n",
    "\n",
    "4 - Parsing XML files using Beautiful Soup -> Group A: \"Neuroscience\" - Group B: \"Cell biology\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####First iteration\n",
    "\n",
    "\n",
    "\n",
    "STEP 2: Retrieve the most frequent terms\n",
    "\n",
    "![Plan chart](img/precursor-doc-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####First iteration\n",
    "\n",
    "\n",
    "STEP 2: Retrieve the most frequent terms\n",
    " \n",
    "-> Use Nltk and Gensim libraries to implement the TF-IDF (Term frequency–inverse document frequency) model\n",
    "\n",
    "->Write functions to be executed on a collection of text files (converted from XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 172 articles in articles/Neuroscience/\n",
      "There are 67 articles in articles/Cell biology/\n",
      "[u'scale', u'colocalization', u'vglut1', u'pearsons', u'coefficient', u'panel', u'red', u'colocalization', u'entire', u'hippocampal']\n"
     ]
    }
   ],
   "source": [
    "# %load get_topics.py\n",
    "\n",
    "#!/home/eleonore/virtenvs/nltk-gensim-skl/bin/python2.7\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string, re, codecs, time, json\n",
    "\n",
    "\n",
    "## Global variables\n",
    "stop_words = ['a', 'also', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'but', 'for',\n",
    "              'from', 'has', 'he', 'if', 'in', 'is', 'it', 'its', 'it\\'s', 'not',\n",
    "              'of', 'on', 'our', 'than', 'that', 'the', 'therefore', 'to', 'was',\n",
    "              'were', 'will', 'with', 'may', 'need', 'have', 'been', 'their', 'this',\n",
    "              'these', 'which', 'do', 'did', 'red', 'blue', 'green', 'bar', 'chart',\n",
    "              'arrowhead', 'arrow', 'vice', 'versa']\n",
    "\n",
    "spe_char = {u'β': 'beta', u'α': 'alpha', u'µm': 'micron'}\n",
    "\n",
    "## Functions to break up the process:\n",
    "def parse_text(text_file):\n",
    "    \"Gets a text file outputs a list of strings.\"\n",
    "    with codecs.open(text_file, mode='r', encoding='utf-8') as f:\n",
    "        read = f.read()\n",
    "        r = [read.replace(unicode(i), spe_char.get(i)) for i in read if i in spe_char.keys()] or [read]\n",
    "        text = [line for line in r[0].strip().split('. ') if line != '']\n",
    "        return text\n",
    "    \n",
    "def get_tokens(text_parsed):\n",
    "    \"Gets a text and retrieves tokens.\"\n",
    "    # Tokenisation\n",
    "    texts = [t.lower().replace('\\n', ' ').split(' ') for t in text_parsed]\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [[filter(lambda x:x not in string.punctuation, i)\n",
    "               for i in txt if i != '' and i not in stop_words] for txt in texts]\n",
    "    \n",
    "    tokens_cleaned = [[i for i in txt if len(i) > 2 and not i.isdigit()] for txt in tokens]\n",
    "    \n",
    "    return tokens_cleaned\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"Gets tokens and retrieves lemmatised tokens.\"\n",
    "    # Lemmatisation using nltk lemmatiser\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma = [[lmtzr.lemmatize(word) for word in data] for data in tokens]\n",
    "    return lemma\n",
    "\n",
    "def bag_of_words(lemma):\n",
    "    \"Takes in lemmatised words and returns a bow.\"\n",
    "    ## Create bag of words from dictionnary\n",
    "    dictionary = Dictionary(lemma)\n",
    "    dictionary.save('text.dict')\n",
    "    ## Term frequency–inverse document frequency (TF-IDF)\n",
    "    bow = [dictionary.doc2bow(l) for l in lemma]\n",
    "    return bow\n",
    "\n",
    "def tfidf_and_lsi(lemma, bow):\n",
    "    \"Gets a bow and returns topics.\"\n",
    "    dictionary = Dictionary(lemma)\n",
    "    # Transform the count representation into the Tfidf space\n",
    "    tfidf = models.TfidfModel(bow) \n",
    "    corpus_tfidf = tfidf[bow]\n",
    "    ## Build the LSI model\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    list_topics = []\n",
    "    for i in range(lsi.num_topics):\n",
    "        list_topics.extend(lsi.show_topic(i))\n",
    "    list_topics.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    topics = [i[1] for i in list_topics[:10]]\n",
    "    return topics\n",
    "\n",
    "## Function to retrieve topics using nltk\n",
    "def get_topics(text_file):\n",
    "    txt = parse_text(text_file)\n",
    "    tokens = get_tokens(txt)\n",
    "    lemma = lemmatize_tokens(tokens)\n",
    "    bow = bag_of_words(lemma)\n",
    "    return tfidf_and_lsi(lemma, bow)\n",
    "\n",
    "## Get all text articles from a path and retrieve topics:\n",
    "def list_all_articles(path):\n",
    "    articles = [f for f in listdir(path) if isfile(join(path, f))] or []\n",
    "    print \"There are %d articles in %s\" % (len(articles), path)\n",
    "    return {\"path\": path, \"articles\": articles}\n",
    "\n",
    "# Write the topics to a json file:\n",
    "#{\"Neuroscience\": {\"pub_id1\":[topics_file1], \"pub_id2\":[topics_file2]...},\n",
    "# \"Cell biology\": {[], []...}}\n",
    "def get_articles_topics(path, filename):\n",
    "    \"Store the topics in a json object and dump to a file.\"\n",
    "    all_topics = {}\n",
    "    #Get the directories in a path\n",
    "    dirs = [d for d in listdir(path) if not isfile(join(path, d))]\n",
    "    #For each dir and for each file in a dir\n",
    "    for d in dirs:\n",
    "        all_topics[d] = {}\n",
    "        txt_files = [f for f in listdir(path+d) if isfile(join(path+d, f))]\n",
    "        print txt_files\n",
    "        for f in txt_files:\n",
    "            all_topics[d][f[:-4]] = get_topics(path+d+'/'+f)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_topics, f)\n",
    "    return all_topics\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    neuro_articles = list_all_articles(\"articles/Neuroscience/\")\n",
    "    \n",
    "    cellbiol_articles = list_all_articles(\"articles/Cell biology/\")\n",
    "    \n",
    "    print get_topics(neuro_articles.get(\"path\") + neuro_articles.get(\"articles\")[0])\n",
    "\n",
    "    #print get_articles_topics(\"articles/\", \"json/filenames_topics.json\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####First iteration\n",
    "\n",
    "\n",
    "STEP 3: Build a classifier to predict the subcategory of an article\n",
    "\n",
    "![Plan chart](img/precursor-doc-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####First iteration\n",
    "\n",
    "\n",
    "STEP 3: Build a classifier to predict the subcategory of an article\n",
    "\n",
    "-> Exploring the data with Numpy and Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'json/filenames_topics.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a2315a50421d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ggplot'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"json/filenames_topics.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtopics_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'json/filenames_topics.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "with open(\"json/filenames_topics.json\", 'r') as f:\n",
    "        topics_articles = json.load(f)\n",
    "        \n",
    "print topics_articles['Neuroscience']['elife05116']\n",
    "\n",
    "print topics_articles['Cell biology']['elife02923']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# List of most frequent terms\n",
    "header = ['article', 'subject']\n",
    "\n",
    "for subject, articles in topics_articles.iteritems():\n",
    "    for pub_id, topics in articles.iteritems():\n",
    "        header.extend(topics)        \n",
    "        \n",
    "print header[:15]\n",
    "\n",
    "# Matrix representing the presence or absence of those terms in each article\n",
    "top_data = []\n",
    "\n",
    "for subject, articles in topics_articles.iteritems():\n",
    "    for pub_id, topics in articles.iteritems():        \n",
    "        ct = []\n",
    "        ct.append(pub_id)\n",
    "        ct.append(subject)\n",
    "        tpcs = ['1' if h in topics else '0' for h in header[2:]]\n",
    "        ct.extend(tpcs)\n",
    "        top_data.append(ct)\n",
    "        \n",
    "print top_data[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use a numpy array (data structure used with Scikit-learn)\n",
    "topics_data = np.array(top_data)\n",
    "print topics_data\n",
    "\n",
    "## Data matrix: column 3 to the end\n",
    "X = topics_data[:, 2:2392].astype(int)\n",
    "\n",
    "## Class vector: column 2\n",
    "Y = topics_data[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print \"\\nX dimension: \", X.shape\n",
    "print \"Y dimension: \", Y.shape\n",
    "\n",
    "Yfreq = sp.stats.itemfreq(Y)\n",
    "print Yfreq\n",
    "cb_total = int(Yfreq[0][1])\n",
    "n_total = int(Yfreq[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "freqs_cb, freqs_n = [], []\n",
    "R, N = range(3, 15, 2), len(range(3, 15, 2))\n",
    "\n",
    "for j in R:\n",
    "    cb = [int(i[1]) for i in topics_data[:, (1, j)] if i[0]=='Cell biology']\n",
    "    n = [int(i[1]) for i in topics_data[:, (1, j)] if i[0]=='Neuroscience']\n",
    "    cb_freq = sum(cb)*100./cb_total\n",
    "    n_freq = sum(n)*100./n_total\n",
    "    freqs_cb.append(cb_freq)\n",
    "    freqs_n.append(n_freq)\n",
    "\n",
    "ind = np.arange(N)\n",
    "width = 0.3\n",
    "fig, ax = plt.subplots()\n",
    "rects_cb = ax.bar(ind, freqs_cb, width, color='#A770BF')\n",
    "rects_n = ax.bar(ind+width, freqs_n, width, color='#81ABD3')\n",
    "\n",
    "ax.set_ylabel('Percentage of occurence')\n",
    "ax.set_title('Occurence of frequent term in Cell biology vs Neuroscience')\n",
    "ax.set_xticks(ind+width)\n",
    "xlabels = [header[i] for i in R]\n",
    "ax.set_xticklabels(xlabels)\n",
    "ax.legend((rects_cb[0], rects_n[0]), ('Cell biology', 'Neuroscience'))\n",
    "\n",
    "fig.subplots_adjust(right=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#x_index, y_index = 48, 350\n",
    "x_index, y_index = 859, 179\n",
    "\n",
    "classes = ['Cell biology', 'Neuroscience']\n",
    "formatter = plt.FuncFormatter(lambda i, *args: classes[int(i)])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "targets = le.transform(Y)\n",
    "\n",
    "plt.scatter(topics_data[:, x_index], topics_data[:, y_index],\n",
    "            s=50, c=targets, alpha=0.5)\n",
    "plt.colorbar(ticks=[0, 1], format=formatter)\n",
    "plt.clim(-0.2, 1.2)\n",
    "plt.xlabel(header[x_index])\n",
    "plt.ylabel(header[y_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####How I executed my plan\n",
    "\n",
    "\n",
    "STEP 3: Build a classifier to predict the subcategory of an article\n",
    "\n",
    "-> Building a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes classifier:\n",
    "\n",
    "* Simple classifier applying Bayes' theorem\n",
    "\n",
    "* Assumes independence of the features\n",
    "\n",
    "* Widely used in different application (text categorisation)\n",
    "\n",
    "* Doesn't require a very large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## NAIVE BAYES classifier:\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "## Encode labels ('Cell biology', 'Neuroscience') as 0 and 1.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "y_transformed = le.transform(Y)\n",
    "#print Y\n",
    "#print y_transformed\n",
    "## => 1: Neuroscience, 0: Cell biology\n",
    "\n",
    "## Randomly split the data between training and testing:\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Bernoulli NB classifier\n",
    "\n",
    "nbmodel_train = BernoulliNB().fit(X_train, Y_train)\n",
    "predicted_bnb = nbmodel_train.predict(X_test)\n",
    "\n",
    "## Confusion matrix:\n",
    "cm_bnb = metrics.confusion_matrix(Y_test, predicted_bnb)\n",
    "print \"True positive: \", cm_bnb[0][0],\" - False negative: \", cm_bnb[0][1]\n",
    "print \"False positive: \", cm_bnb[1][0], \" - True negative: \", cm_bnb[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Bernoulli NB classifier\n",
    "\n",
    "# Precision: fraction of retrieved instances that are relevant (TP / TP + FP)\n",
    "# Recall: fraction of relevant instances that are retrieved\n",
    "print metrics.classification_report(Y_test, predicted_bnb)\n",
    "\n",
    "# Accuracy: overall correctness of the model\n",
    "print \"Accuracy: \", metrics.accuracy_score(Y_test, predicted_bnb) \n",
    "\n",
    "## => Test with a high precision and recall for class 1 = Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "K-nearest neighbor algorithm:\n",
    "\n",
    "* Predict the label of a new point from its neighbours\n",
    "\n",
    "* Simplest classifier in Machine Learning\n",
    "\n",
    "* Lazy learner: computationally demanding for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## K NEAREST NEIGHBOUR classifier\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y_transformed)\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=3).fit(X_train, Y_train)\n",
    "predicted_knn = knn.predict(X_test)\n",
    "\n",
    "cm_knn = metrics.confusion_matrix(Y_test, predicted_knn)\n",
    "print \"True positive: %d - False negative: %d\" % (cm_knn[0][0], cm_knn[0][1])\n",
    "print \"False positive: %d - True negative: %d \\n\" % (cm_knn[1][0], cm_knn[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## K NEAREST NEIGHBOUR classifier\n",
    "\n",
    "# Precision: fraction of retrieved instances that are relevant (TP / TP + FP)\n",
    "# Recall: fraction of relevant instances that are retrieved\n",
    "print metrics.classification_report(Y_test, predicted_knn)\n",
    "\n",
    "# Accuracy: overall correctness of the model\n",
    "print \"Accuracy: \", metrics.accuracy_score(Y_test, predicted_knn) \n",
    "\n",
    "## => Test with a high precision and recall for class 1 = Neuroscience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Model validation: test several training/testing data\n",
    "# K fold cross validation: iterator that randomise the sampling\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "kf = range(2, 21)\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "nb = BernoulliNB()\n",
    "\n",
    "means_knn, sds_knn, means_nb, sds_nb = [], [], [], []\n",
    "\n",
    "for k in kf:\n",
    "    knn_scores = cross_val_score(knn, X_train, Y_train, cv=k)\n",
    "    nb_scores = cross_val_score(nb, X_train, Y_train, cv=k)\n",
    "    means_knn.append(knn_scores.mean()) \n",
    "    sds_knn.append(knn_scores.std())\n",
    "    means_nb.append(nb_scores.mean())\n",
    "    sds_nb.append(nb_scores.std())\n",
    "\n",
    "print means_knn\n",
    "print sds_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(kf, means_knn, label=\"KNN 3 mean accuracy\", color=\"purple\")\n",
    "plt.plot(kf, means_nb, label=\"NB mean accuracy\", color=\"blue\")\n",
    "plt.legend(loc=2)\n",
    "plt.ylim(0.6, 0.9)\n",
    "plt.title(\"Accuracy means with increasing K-fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(kf, sds_knn, label=\"KNN 3 sds accuracy\", color=\"purple\")\n",
    "plt.plot(kf, sds_nb, label=\"NB sds accuracy\", color=\"blue\")\n",
    "plt.legend(loc=2)\n",
    "plt.ylim(0.01, 0.4)\n",
    "plt.title(\"Accuracy standard deviation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 10 folds\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y_transformed)\n",
    "\n",
    "NB_train = BernoulliNB()\n",
    "for i in range(10):\n",
    "    NB_train.fit(X_train, Y_train)\n",
    "    \n",
    "import get_topics as gt\n",
    "\n",
    "topics_test1 = get_topics(\"articles/conrad2013_melanoma.txt\")\n",
    "print topics_test1\n",
    "\n",
    "topics_test2 = get_topics(\"articles/panizzo2014_cerebral_ischaemia.txt\")\n",
    "print topics_test2\n",
    "\n",
    "test1 = [1 if h in topics_test1 else 0 for h in header[2:]]\n",
    "test2 = [1 if h in topics_test2 else 0 for h in header[2:]]\n",
    "\n",
    "NB_test1 = NB_train.predict(test1)\n",
    "print NB_test1\n",
    "\n",
    "NB_test2 = NB_train.predict(test2)\n",
    "print NB_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Getting meaning from scientific articles\n",
    "\n",
    "1- Who I am\n",
    "\n",
    "2- Goal of this project\n",
    "\n",
    "3- My plan\n",
    "\n",
    "4- First iteration\n",
    "\n",
    "__5- Improvements__\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "7- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Improvements\n",
    "\n",
    "\n",
    "\n",
    "* Use article abstracts only\n",
    "\n",
    "\n",
    "* Use Python 3 to avoid unicode issues\n",
    "\n",
    "\n",
    "* Use the Latent Dirichet Allocation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Second iteration\n",
    "\n",
    "\n",
    "__Topic modeling__\n",
    "\n",
    ">\"A __topic model__ is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents.\"\n",
    "\n",
    "> http://en.wikipedia.org/wiki/Topic_model\n",
    "\n",
    "* Latent Semantic Analysis (LSA)\n",
    "\n",
    "-> Analyse relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
    "\n",
    "\n",
    "* Latent Derichelet Allocation (LDA)\n",
    "\n",
    "-> Differs from LSA by assuming each document to be characterized by a mixture of various topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Second iteration\n",
    "\n",
    "__Latent Derichelet Allocation (LDA) with Gensim__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import string \n",
    "import codecs\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "# Testing data: 2 Cell bio article abstracts\n",
    "\n",
    "conrad2013 = \"\"\"The inability of targeted BRAF inhibitors to produce long-lasting improvement in the clinical outcome of melanoma highlights a need to identify additional approaches to inhibit melanoma growth. \n",
    "            Recent studies have shown that activation of the Wnt/β-catenin pathway decreases tumor growth and cooperates with ERK/MAPK pathway inhibitors to promote apoptosis in melanoma. \n",
    "            Therefore, the identification of Wnt/β-catenin regulators may advance the development of new approaches to treat this disease. \n",
    "            In order to move towards this goal we performed a large scale small-interfering RNA (siRNA) screen for regulators of β-catenin activated reporter activity in human HT1080 fibrosarcoma cells. \n",
    "            Integrating large scale siRNA screen data with phosphoproteomic data and bioinformatics enrichment identified a protein, FAM129B, as a potential regulator of Wnt/β-catenin signaling.  \n",
    "            Functionally, we demonstrated that siRNA-mediated knockdown of FAM129B in A375 and A2058 melanoma cell lines inhibits WNT3A-mediated activation of a β-catenin-responsive luciferase reporter and inhibits expression of the endogenous Wnt/β-catenin target gene, AXIN2. \n",
    "            We also demonstrate that FAM129B knockdown inhibits apoptosis in melanoma cells treated with WNT3A. These experiments support a role for FAM129B in linking Wnt/β-catenin signaling to apoptosis in melanoma.\n",
    "            The incidence of melanoma continues to rise across the U.S. at a rate faster than any other cancer. Malignant melanoma has a poor prognosis with a 5-year survival rate of only 15%3. \n",
    "            The recently approved therapeutic, vemurafenib, extends median patient survival by 7 months. This major advance raises expectations that even greater rates of survival might be attainable with combination therapies.\"\"\"\n",
    "\n",
    "huang2015 = \"\"\"Mouse GnT1IP-L, and membrane-bound GnT1IP-S (MGAT4D) expressed in cultured cells inhibit MGAT1, the N-acetylglucosaminyltransferase that initiates the synthesis of hybrid and complex N-glycans. \n",
    "            However, it is not known where in the secretory pathway GnT1IP-L inhibits MGAT1, nor whether GnT1IP-L inhibits other N-glycan branching N-acetylglucosaminyltransferases of the medial Golgi. We show here that the luminal domain of GnT1IP-L contains its inhibitory activity. \n",
    "            Retention of GnT1IP-L in the endoplasmic reticulum (ER) via the N-terminal region of human invariant chain p33, with or without C-terminal KDEL, markedly reduced inhibitory activity. \n",
    "            Dynamic fluorescent resonance energy transfer (FRET) and bimolecular fluorescence complementation (BiFC) assays revealed homomeric interactions for GnT1IP-L in the ER, and heteromeric interactions with MGAT1 in the Golgi. \n",
    "            GnT1IP-L did not generate a FRET signal with MGAT2, MGAT3, MGAT4B or MGAT5 medial Golgi GlcNAc-tranferases. \n",
    "            GnT1IP/Mgat4d transcripts are expressed predominantly in spermatocytes and spermatids in mouse, and are reduced in men with impaired spermatogenesis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The inability of targeted BRAF inhibitors to produce long-lasting improvement in the clinical outcome of melanoma highlights a need to identify additional approaches to inhibit melanoma growth. \\n            Recent studies have shown that activation of the Wnt/\\xce\\xb2-catenin pathway decreases tumor growth and cooperates with ERK/MAPK pathway inhibitors to promote apoptosis in melanoma. \\n            Therefore, the identification of Wnt/\\xce\\xb2-catenin regulators may advance the development of new approaches to treat this disease. \\n            In order to move towards this goal we performed a large scale small-interfering RNA (siRNA) screen for regulators of \\xce\\xb2-catenin activated reporter activity in human HT1080 fibrosarcoma cells. \\n            Integrating large scale siRNA screen data with phosphoproteomic data and bioinformatics enrichment identified a protein, FAM129B, as a potential regulator of Wnt/\\xce\\xb2-catenin signaling.  \\n            Functionally, we demonstrated that siRNA-mediated knockdown of FAM129B in A375 and A2058 melanoma cell lines inhibits WNT3A-mediated activation of a \\xce\\xb2-catenin-responsive luciferase reporter and inhibits expression of the endogenous Wnt/\\xce\\xb2-catenin target gene, AXIN2. \\n            We also demonstrate that FAM129B knockdown inhibits apoptosis in melanoma cells treated with WNT3A. These experiments support a role for FAM129B in linking Wnt/\\xce\\xb2-catenin signaling to apoptosis in melanoma.\\n            The incidence of melanoma continues to rise across the U.S. at a rate faster than any other cancer. Malignant melanoma has a poor prognosis with a 5-year survival rate of only 15%3. \\n            The recently approved therapeutic, vemurafenib, extends median patient survival by 7 months. This major advance raises expectations that even greater rates of survival might be attainable with combination therapies.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:5: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "spe_char = {u'β': 'beta', u'α': 'alpha', u'µm': 'micron'}\n",
    "\n",
    "def parse_text(text):\n",
    "    \"Gets a text and outputs a list of strings.\"\n",
    "    doc = [text.replace(unicode(i), spe_char.get(i)) for i in text if i in spe_char.keys()] or [text]\n",
    "    return doc\n",
    "\n",
    "conrad2013_parsed = parse_text(conrad2013)\n",
    "\n",
    "huang2015_parsed = parse_text(huang2015)\n",
    "\n",
    "print conrad2013_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inability', 'targeted', 'braf', 'inhibitors', 'produce', 'long', 'lasting', 'improvement', 'clinical', 'outcome', 'melanoma', 'highlights', 'need', 'identify', 'additional', 'approaches', 'inhibit', 'melanoma', 'growth', 'recent', 'studies', 'shown', 'activation', 'wnt', 'catenin', 'pathway', 'decreases', 'tumor', 'growth', 'cooperates', 'erk', 'mapk', 'pathway', 'inhibitors', 'promote', 'apoptosis', 'melanoma', 'therefore', 'identification', 'wnt', 'catenin', 'regulators', 'advance', 'development', 'new', 'approaches', 'treat', 'disease', 'order', 'goal', 'performed', 'large', 'scale', 'small', 'interfering', 'rna', 'sirna', 'screen', 'regulators', 'catenin', 'activated', 'reporter', 'activity', 'human', 'ht1080', 'fibrosarcoma', 'cells', 'integrating', 'large', 'scale', 'sirna', 'screen', 'data', 'phosphoproteomic', 'data', 'bioinformatics', 'enrichment', 'identified', 'protein', 'fam129b', 'potential', 'regulator', 'wnt', 'catenin', 'signaling', 'functionally', 'demonstrated', 'sirna', 'mediated', 'knockdown', 'fam129b', 'a375', 'a2058', 'melanoma', 'cell', 'lines', 'inhibits', 'wnt3a', 'mediated', 'activation', 'catenin', 'responsive', 'luciferase', 'reporter', 'inhibits', 'expression', 'endogenous', 'wnt', 'catenin', 'target', 'gene', 'axin2', 'demonstrate', 'fam129b', 'knockdown', 'inhibits', 'apoptosis', 'melanoma', 'cells', 'treated', 'wnt3a', 'experiments', 'support', 'role', 'fam129b', 'linking', 'wnt', 'catenin', 'signaling', 'apoptosis', 'melanoma', 'incidence', 'melanoma', 'continues', 'rise', 'rate', 'faster', 'cancer', 'malignant', 'melanoma', 'poor', 'prognosis', 'year', 'survival', 'rate', 'recently', 'approved', 'therapeutic', 'vemurafenib', 'extends', 'median', 'patient', 'survival', 'months', 'major', 'advance', 'raises', 'expectations', 'greater', 'rates', 'survival', 'attainable', 'combination', 'therapies']\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(text_parsed):\n",
    "    \"Gets a text and retrieves tokens.\"\n",
    "    # Tokenisation\n",
    "    texts = text_parsed[0].lower().replace('\\n', ' ').replace('/', ' ').replace('-', ' ').split(' ')\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [filter(lambda x:x not in string.punctuation, i)\n",
    "               for i in texts if i != '' and i not in STOPWORDS]\n",
    "    tokens_cleaned = [i for i in tokens if len(i) > 2 and not i.isdigit()]\n",
    "    return tokens_cleaned\n",
    "\n",
    "conrad2013_tokens = get_tokens(conrad2013_parsed)\n",
    "\n",
    "huang2015_tokens = get_tokens(huang2015_parsed)\n",
    "\n",
    "print conrad2013_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inability', 'targeted', 'braf', u'inhibitor', 'produce', 'long', 'lasting', 'improvement', 'clinical', 'outcome', 'melanoma', u'highlight', 'need', 'identify', 'additional', u'approach', 'inhibit', 'melanoma', 'growth', 'recent', u'study', 'shown', 'activation', 'wnt', 'catenin', 'pathway', u'decrease', 'tumor', 'growth', 'cooperates', 'erk', 'mapk', 'pathway', u'inhibitor', 'promote', 'apoptosis', 'melanoma', 'therefore', 'identification', 'wnt', 'catenin', u'regulator', 'advance', 'development', 'new', u'approach', 'treat', 'disease', 'order', 'goal', 'performed', 'large', 'scale', 'small', 'interfering', 'rna', 'sirna', 'screen', u'regulator', 'catenin', 'activated', 'reporter', 'activity', 'human', 'ht1080', 'fibrosarcoma', u'cell', 'integrating', 'large', 'scale', 'sirna', 'screen', 'data', 'phosphoproteomic', 'data', 'bioinformatics', 'enrichment', 'identified', 'protein', 'fam129b', 'potential', 'regulator', 'wnt', 'catenin', 'signaling', 'functionally', 'demonstrated', 'sirna', 'mediated', 'knockdown', 'fam129b', 'a375', 'a2058', 'melanoma', 'cell', u'line', 'inhibits', 'wnt3a', 'mediated', 'activation', 'catenin', 'responsive', 'luciferase', 'reporter', 'inhibits', 'expression', 'endogenous', 'wnt', 'catenin', 'target', 'gene', 'axin2', 'demonstrate', 'fam129b', 'knockdown', 'inhibits', 'apoptosis', 'melanoma', u'cell', 'treated', 'wnt3a', u'experiment', 'support', 'role', 'fam129b', 'linking', 'wnt', 'catenin', 'signaling', 'apoptosis', 'melanoma', 'incidence', 'melanoma', 'continues', 'rise', 'rate', 'faster', 'cancer', 'malignant', 'melanoma', 'poor', 'prognosis', 'year', 'survival', 'rate', 'recently', 'approved', 'therapeutic', 'vemurafenib', 'extends', 'median', 'patient', 'survival', u'month', 'major', 'advance', u'raise', u'expectation', 'greater', u'rate', 'survival', 'attainable', 'combination', u'therapy']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"Gets tokens and retrieves lemmatised tokens.\"\n",
    "    # Lemmatisation using nltk lemmatiser\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma = [lmtzr.lemmatize(t) for t in tokens]\n",
    "    return lemma\n",
    "\n",
    "conrad2013_lemma = lemmatize_tokens(conrad2013_tokens)\n",
    "\n",
    "huang2015_lemma = lemmatize_tokens(huang2015_tokens)\n",
    "\n",
    "print conrad2013_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words representation of the first document (tuples are composed by token_id and multiplicity):\n",
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 2), (7, 3), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 7), (16, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 4), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 3), (54, 1), (55, 1), (56, 2), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 2), (68, 8), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 2), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 3), (86, 1), (87, 1), (88, 3), (89, 2), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 2), (96, 1), (97, 2), (98, 3), (99, 1), (100, 1), (101, 1), (102, 3), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 5), (113, 2), (114, 1)]\n",
      "\n",
      "In the document, topic_id 0 (word inability) appears 1 time[s]\n",
      "In the document, topic_id 1 (word targeted) appears 1 time[s]\n",
      "In the document, topic_id 2 (word braf) appears 1 time[s]\n",
      "In the document, topic_id 3 (word inhibitor) appears 2 time[s]\n",
      "In the document, topic_id 4 (word produce) appears 1 time[s]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(lemma):\n",
    "    \"Takes in lemmatised words and returns a bow.\"\n",
    "    # Create bag of words from dictionnary\n",
    "    dictionary = Dictionary(lemma)\n",
    "    dictionary.save('text.dict')\n",
    "    # Term frequency–inverse document frequency (TF-IDF)\n",
    "    bow = [dictionary.doc2bow(l) for l in lemma] # Calculates inverse document counts for all terms\n",
    "    return (bow, dictionary)\n",
    "\n",
    "\n",
    "docs = [conrad2013_lemma, huang2015_lemma] # testing corpus\n",
    "\n",
    "bow, dictionary = bag_of_words(docs)\n",
    "\n",
    "print \"Bag of words representation of the first document (tuples are composed by token_id and multiplicity):\\n\", bow[0]\n",
    "print\n",
    "for i in range(5):\n",
    "    print \"In the document, topic_id %d (word %s) appears %d time[s]\" %(bow[0][i][0], docs[0][bow[0][i][0]], bow[0][i][1])\n",
    "print \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.990722\n",
      "Topic: 0.080*gnt1ip + 0.027*golgi + 0.027*mgat1 + 0.018*activity + 0.018*reduced + 0.018*inhibitory + 0.018*interaction + 0.018*fret + 0.018*expressed + 0.018*medial\n"
     ]
    }
   ],
   "source": [
    "def lda_model(dictionary, bow):\n",
    "    ldamodel = models.ldamodel.LdaModel(bow, num_topics=10, id2word=dictionary, passes=5)\n",
    "    return ldamodel\n",
    "\n",
    "\n",
    "lda = lda_model(dictionary, bow)\n",
    "\n",
    "for index, score in sorted(lda[bow[1]], key=lambda tup: -1*tup[1]):\n",
    "    print \"Score: %f\\nTopic: %s\" %(score, lda.print_topic(index, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.610076\n",
      " Topic: 0.044*melanoma + 0.039*catenin + 0.028*wnt + 0.023*fam129b + 0.017*cell\n",
      "Score: 0.328385\n",
      " Topic: 0.080*gnt1ip + 0.027*golgi + 0.027*mgat1 + 0.018*activity + 0.018*reduced\n"
     ]
    }
   ],
   "source": [
    "# Try the model on another article abstract:\n",
    "\n",
    "burns2015 = \"\"\"Duplication of the yeast centrosome (called the spindle pole body, SPB) is thought to occur through a series of discrete steps that culminate in insertion of the new SPB into the nuclear envelope (NE). \n",
    "            To better understand this process, we developed a novel two-color structured illumination microscopy with single-particle averaging (SPA-SIM) approach to study the localization of all 18 SPB components during duplication using endogenously expressed fluorescent protein derivatives. \n",
    "            The increased resolution and quantitative intensity information obtained using this method allowed us to demonstrate that SPB duplication begins by formation of an asymmetric Sfi1 filament at mitotic exit followed by Mps1-dependent assembly of a Spc29- and Spc42-dependent complex at its tip. \n",
    "            Our observation that proteins involved in membrane insertion, such as Mps2, Bbp1, and Ndc1, also accumulate at the new SPB early in duplication suggests that SPB assembly and NE insertion are coupled events during SPB formation in wild-type cells.\"\"\"\n",
    "\n",
    "burns2015_tokens = lemmatize_tokens(get_tokens(parse_text(burns2015)))\n",
    "\n",
    "bow_vector = dictionary.doc2bow(burns2015_tokens)\n",
    "for index, score in sorted(lda[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print \"Score: %f\\n Topic: %s\" %(score, lda.print_topic(index, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67 articles in this folder.\n",
      "\n",
      "Article elife00772.txt\n",
      " Score: 0.999293 - Topic: 0.045*cell + 0.013*cln3 + 0.007*membrane + 0.007*force + 0.007*1—figure + 0.007*supplement + 0.007*length + 0.006*whi5 + 0.006*http + 0.006*dxdoiorg\n",
      "\n",
      "Article elife00654.txt\n",
      " Score: 0.999173 - Topic: 0.025*protein + 0.017*cell + 0.013*ift + 0.010*membrane + 0.009*cilium + 0.009*golgi + 0.008*cargo + 0.007*ciliary + 0.006*transport + 0.006*gfp\n",
      "\n",
      "Article elife02137.txt\n",
      " Score: 0.996547 - Topic: 0.027*snap + 0.026*cell + 0.016*alpha + 0.012*rnai + 0.010*protein + 0.010*soce + 0.009*figure + 0.009*cenp + 0.008*orai1 + 0.007*stim1\n",
      "\n",
      "Article elife03083.txt\n",
      " Score: 0.998843 - Topic: 0.024*cell + 0.020*cavin + 0.009*bpac + 0.009*cilium + 0.008*figure + 0.008*signaling + 0.007*serum + 0.007*apc + 0.007*knockdown + 0.007*anti\n",
      "\n",
      "Article elife06156.txt\n",
      " Score: 0.998976 - Topic: 0.045*cell + 0.013*cln3 + 0.007*membrane + 0.007*force + 0.007*1—figure + 0.007*supplement + 0.007*length + 0.006*whi5 + 0.006*http + 0.006*dxdoiorg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    # Clean\n",
    "    doc = [text.replace(unicode(i), spe_char.get(i)) for i in text if i in spe_char.keys()] or [text]\n",
    "    # Tokenise\n",
    "    texts = doc[0].lower().replace('\\n', ' ').replace('/', ' ').replace('-', ' ').split(' ')\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [filter(lambda x:x not in string.punctuation, i)\n",
    "               for i in texts if i != '' and i not in STOPWORDS]\n",
    "    tokens_cleaned = [i for i in tokens if len(i) > 2 and not i.isdigit()]\n",
    "    # Lemmatise\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmatised_text = [lmtzr.lemmatize(t) for t in tokens_cleaned]\n",
    "    return lemmatised_text\n",
    "\n",
    "\n",
    "def parse_from_file(text_file):\n",
    "    \"Retrieves the content of a text file\"\n",
    "    with codecs.open(text_file, mode='r', encoding='utf-8') as f:\n",
    "        reader = f.read()\n",
    "        return reader\n",
    "\n",
    "    \n",
    "from os import listdir\n",
    "\n",
    "# Gather the articles\n",
    "filepath = \"articles/Cell biology/\"\n",
    "articles = [f for f in listdir(filepath) if isfile(join(filepath, f))] or []\n",
    "print \"There are %d articles in this folder.\\n\" % len(articles)\n",
    "\n",
    "# Create a corpus\n",
    "corpus = [prepare_text_for_lda(parse_from_file(filepath + a)) for a in articles[:45]]\n",
    "\n",
    "articles_bow, articles_dict = bag_of_words(corpus)\n",
    "\n",
    "articles_lda = lda_model(articles_dict, articles_bow)\n",
    "\n",
    "for i in range(5):\n",
    "    for index, score in sorted(articles_lda[articles_bow[i]], key=lambda tup: -1*tup[1]):\n",
    "        print \"Article %s\\n Score: %f - Topic: %s\\n\" %(articles[i], score, articles_lda.print_topic(index, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article de Bio Cell:\n",
      "elife03307.txt\n",
      "\n",
      "Score: 0.999176\n",
      "Topic: 0.020*cell + 0.014*protein + 0.007*membrane + 0.007*glucose + 0.006*alpha + 0.006*figure + 0.006*gfp + 0.006*tnf + 0.005*rod1 + 0.005*endocytosis\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Article de Neuro:\n",
      "elife05438.txt\n",
      "\n",
      "Score: 0.239852\n",
      "Topic: 0.025*protein + 0.017*cell + 0.013*ift + 0.010*membrane + 0.009*cilium + 0.009*golgi + 0.008*cargo + 0.007*ciliary + 0.006*transport + 0.006*gfp\n",
      "\n",
      "Score: 0.200489\n",
      "Topic: 0.045*cell + 0.013*cln3 + 0.007*membrane + 0.007*force + 0.007*1—figure + 0.007*supplement + 0.007*length + 0.006*whi5 + 0.006*http + 0.006*dxdoiorg\n",
      "\n",
      "Score: 0.137327\n",
      "Topic: 0.027*snap + 0.026*cell + 0.016*alpha + 0.012*rnai + 0.010*protein + 0.010*soce + 0.009*figure + 0.009*cenp + 0.008*orai1 + 0.007*stim1\n",
      "\n",
      "Score: 0.135560\n",
      "Topic: 0.043*cell + 0.011*pom1 + 0.009*figure + 0.008*protein + 0.008*pom1p + 0.007*strain + 0.007*intensity + 0.006*time + 0.006*length + 0.006*size\n",
      "\n",
      "Score: 0.108637\n",
      "Topic: 0.020*cell + 0.014*protein + 0.007*membrane + 0.007*glucose + 0.006*alpha + 0.006*figure + 0.006*gfp + 0.006*tnf + 0.005*rod1 + 0.005*endocytosis\n",
      "\n",
      "Score: 0.075220\n",
      "Topic: 0.016*cell + 0.014*ap2 + 0.014*protein + 0.011*fcho + 0.008*membrane + 0.008*zbtb16 + 0.007*cargo + 0.007*figure + 0.006*autophagy + 0.006*mutant\n",
      "\n",
      "Score: 0.064887\n",
      "Topic: 0.024*cell + 0.020*cavin + 0.009*bpac + 0.009*cilium + 0.008*figure + 0.008*signaling + 0.007*serum + 0.007*apc + 0.007*knockdown + 0.007*anti\n",
      "\n",
      "Score: 0.030209\n",
      "Topic: 0.034*micos + 0.021*complex + 0.018*qil1 + 0.017*protein + 0.014*figure + 0.013*mitochondrial + 0.013*subunit + 0.012*analysis + 0.011*membrane + 0.011*cristae\n"
     ]
    }
   ],
   "source": [
    "print \"Article de Bio Cell:\"\n",
    "cb_article = articles[50]\n",
    "print test_article\n",
    "\n",
    "cb_tokens = prepare_text_for_lda(parse_from_file(filepath + cb_article))\n",
    "\n",
    "cb_bow = articles_dict.doc2bow(cb_tokens)\n",
    "\n",
    "for index, score in sorted(articles_lda[cb_bow], key=lambda tup: -1*tup[1]):\n",
    "    print \"\\nScore: %f\\nTopic: %s\\n\" %(score, articles_lda.print_topic(index, 10))\n",
    "    \n",
    "print \"-\" * 90\n",
    "print \"Article de Neuro:\"  \n",
    "filepath_ns = \"articles/Neuroscience/\"\n",
    "articles_ns = [f for f in listdir(filepath_ns) if isfile(join(filepath_ns, f))] or []\n",
    "\n",
    "ns_article = articles_ns[0]\n",
    "print ns_article\n",
    "\n",
    "ns_tokens = prepare_text_for_lda(parse_from_file(filepath_ns + ns_article))\n",
    "\n",
    "ns_bow = articles_dict.doc2bow(ns_tokens)\n",
    "\n",
    "for index, score in sorted(articles_lda[ns_bow], key=lambda tup: -1*tup[1]):\n",
    "    print \"\\nScore: %f\\nTopic: %s\" %(score, articles_lda.print_topic(index, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1- Who I am\n",
    "\n",
    "2- Goal of this project\n",
    "\n",
    "3- My plan\n",
    "\n",
    "4- First iteration\n",
    "\n",
    "5- Improvements\n",
    "\n",
    "6- Second iteration\n",
    "\n",
    "__7- Discussion__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####Discussion\n",
    "\n",
    "State of the project:\n",
    "\n",
    "* LDA seems like a good tool to compare articles on their content\n",
    "\n",
    "* Next step: train lda model on different types of articles\n",
    "\n",
    "* Idea: use the number of topics to evalute the proximity with a subject\n",
    "\n",
    "_______\n",
    "\n",
    "\n",
    "Tools used:\n",
    "\n",
    "* Python 2.7 and iPython notebook\n",
    "* Nltk library for natural text processing (nlp)\n",
    "* Gensim library for nlp and topic modeling\n",
    "* Scikit-learn library for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##That's all folks!\n",
    "\n",
    "<br/>\n",
    "####Questions and suggestions welcome =)\n",
    "\n",
    "<br/>\n",
    "####@EleonoreMayola\n",
    "<br/>\n",
    "github.com/Eleonore9/get-articles-meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<style>\n",
       "\n",
       ".rendered_html {\n",
       "    font-family: \"proxima-nova\", helvetica;\n",
       "    font-size: 120%;\n",
       "    line-height: 1.3;\n",
       "}\n",
       "\n",
       ".rendered_html h1 {\n",
       "    margin: 0.25em 0em 0.5em;\n",
       "    color: #349675;\n",
       "    text-align: center;\n",
       "    line-height: 1.2; \n",
       "    page-break-before: always;\n",
       "}\n",
       "\n",
       ".rendered_html h2 {\n",
       "    margin: 1.1em 0em 0.5em;\n",
       "    color: #415EBF;\n",
       "    text-align: center;\n",
       "    line-height: 1.2;\n",
       "}\n",
       "\n",
       ".rendered_html h4 {\n",
       "    margin-top: 1.2em;\n",
       "    margin-bottom: 1.1em;\n",
       "    color: #878383;\n",
       "    line-height: 1.2;\n",
       "}\n",
       "\n",
       "h4.center {\n",
       "    margin-left: 13em;\n",
       "}\n",
       "\n",
       "h4.left {\n",
       "    margin-left: 9em;\n",
       "}\n",
       "\n",
       ".rendered_html p.ref {\n",
       "    font-size: 70%;\n",
       "}\n",
       "\n",
       ".rendered_html h5 {\n",
       "    margin: 1.1em 0em 0.5em;\n",
       "    /*text-align: center;*/\n",
       "    line-height: 1.1;\n",
       "}\n",
       "\n",
       ".rendered_html li {\n",
       "    line-height: 1.5; \n",
       "}\n",
       "\n",
       ".rendered_html p {\n",
       "    /*text-align: center;*/\n",
       "}\n",
       ".rendered_html span {\n",
       "    text-align: center;\n",
       "    line-height: 0.5;\n",
       "}\n",
       ".prompt {\n",
       "    font-size: 120%; \n",
       "}\n",
       "\n",
       ".img_center {\n",
       "    border-left: 200px;\n",
       "}\n",
       "\n",
       ".CodeMirror-lines {\n",
       "    font-size: 120%; \n",
       "}\n",
       "\n",
       ".output_area {\n",
       "    font-size: 120%; \n",
       "}\n",
       "\n",
       "#notebook {\n",
       "    \n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "\n",
    "<style>\n",
    "\n",
    ".rendered_html {\n",
    "    font-family: \"proxima-nova\", helvetica;\n",
    "    font-size: 120%;\n",
    "    line-height: 1.3;\n",
    "}\n",
    "\n",
    ".rendered_html h1 {\n",
    "    margin: 0.25em 0em 0.5em;\n",
    "    color: #349675;\n",
    "    text-align: center;\n",
    "    line-height: 1.2; \n",
    "    page-break-before: always;\n",
    "}\n",
    "\n",
    ".rendered_html h2 {\n",
    "    margin: 1.1em 0em 0.5em;\n",
    "    color: #415EBF;\n",
    "    text-align: center;\n",
    "    line-height: 1.2;\n",
    "}\n",
    "\n",
    ".rendered_html h4 {\n",
    "    margin-top: 1.2em;\n",
    "    margin-bottom: 1.1em;\n",
    "    color: #878383;\n",
    "    line-height: 1.2;\n",
    "}\n",
    "\n",
    "h4.center {\n",
    "    margin-left: 13em;\n",
    "}\n",
    "\n",
    "h4.left {\n",
    "    margin-left: 9em;\n",
    "}\n",
    "\n",
    ".rendered_html p.ref {\n",
    "    font-size: 70%;\n",
    "}\n",
    "\n",
    ".rendered_html h5 {\n",
    "    margin: 1.1em 0em 0.5em;\n",
    "    /*text-align: center;*/\n",
    "    line-height: 1.1;\n",
    "}\n",
    "\n",
    ".rendered_html li {\n",
    "    line-height: 1.5; \n",
    "}\n",
    "\n",
    ".rendered_html p {\n",
    "    /*text-align: center;*/\n",
    "}\n",
    ".rendered_html span {\n",
    "    text-align: center;\n",
    "    line-height: 0.5;\n",
    "}\n",
    ".prompt {\n",
    "    font-size: 120%; \n",
    "}\n",
    "\n",
    ".img_center {\n",
    "    border-left: 200px;\n",
    "}\n",
    "\n",
    ".CodeMirror-lines {\n",
    "    font-size: 120%; \n",
    "}\n",
    "\n",
    ".output_area {\n",
    "    font-size: 120%; \n",
    "}\n",
    "\n",
    "#notebook {\n",
    "    \n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "display(HTML(s))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
