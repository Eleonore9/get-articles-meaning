{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting meaning from a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Using TextBlob, NLTK and Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example text:\n",
    "zen = [\"Beautiful is better than ugly. Explicit is better than implicit.\", \n",
    "        \"Simple is better than complex. Complex is better than complicated.\",\n",
    "        \"Flat is better than nested. Sparse is better than dense.\",\n",
    "        \"Readability counts. Special cases aren't special enough to break the rules.\",\n",
    "        \"Although practicality beats purity. Errors should never pass silently.\",\n",
    "        \"Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess.\" ,\n",
    "        \"There should be one-- and preferably only one --obvious way to do it.\",\n",
    "        \"Although that way may not be obvious at first unless you're Dutch.\",\n",
    "        \"Now is better than never. Although never is often better than *right* now.\",\n",
    "        \"If the implementation is hard to explain, it's a bad idea.\",\n",
    "        \"If the implementation is easy to explain, it may be a good idea.\"\n",
    "        \"Namespaces are one honking great idea -- let's do more of those!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', 'counts', 'special', 'cases', 'arent', 'special', 'enough', 'break', 'rules'], ['although', 'practicality', 'beats', 'purity', 'errors', 'should', 'never', 'pass', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'its', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', 'lets', 'do', 'more', 'those']]\n",
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', 'counts', 'special', 'cases', 'arent', 'special', 'enough', 'break', 'rules'], ['although', 'practicality', 'beats', 'purity', 'errors', 'should', 'never', 'pass', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', 'lets', 'do', 'more', 'those']]\n"
     ]
    }
   ],
   "source": [
    "## TOKENISATION:\n",
    "## Breaking down a text in meaningful elements\n",
    "texts = [text.lower().replace('\\n', ' ').split(' ') for text in zen]\n",
    "print texts\n",
    "\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for'\n",
    "             'from', 'has', 'he', 'if', 'in', 'is', 'it', 'its', 'it\\'s', 'of', 'on', \n",
    "             'than', 'that', 'the', 'to', 'was', 'were', 'will', 'with']\n",
    "\n",
    "docs = [[filter(lambda x:x not in string.punctuation, i) for i in txt if i != '' and i not in stop_words] \n",
    "        for txt in texts]\n",
    "print docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly', 'explicit', 'better', 'implicit'], ['simple', 'better', 'complex', 'complex', 'better', 'complicated'], ['flat', 'better', 'nested', 'sparse', 'better', 'dense'], ['readability', u'count', 'special', u'case', 'arent', 'special', 'enough', 'break', u'rule'], ['although', 'practicality', u'beat', 'purity', u'error', 'should', 'never', u'pas', 'silently'], ['unless', 'explicitly', 'silenced', 'face', 'ambiguity', 'refuse', 'temptation', 'guess'], ['there', 'should', 'one', 'preferably', 'only', 'one', 'obvious', 'way', 'do', 'it'], ['although', 'way', 'may', 'not', 'obvious', 'first', 'unless', 'youre', 'dutch'], ['now', 'better', 'never', 'although', 'never', 'often', 'better', 'right', 'now'], ['implementation', 'hard', 'explain', 'bad', 'idea'], ['implementation', 'easy', 'explain', 'may', 'good', 'ideanamespaces', 'one', 'honking', 'great', 'idea', '', u'let', 'do', 'more', 'those']]\n"
     ]
    }
   ],
   "source": [
    "## LEMMATISATION:\n",
    "## Grouping together the different forms of a word\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemm = [[lmtzr.lemmatize(word) for word in data] for data in docs]\n",
    "print lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create bag of words from dictionnary:\n",
    "####Note: compare doc2bow and word2vec\n",
    "dictionary = Dictionary(lemm)\n",
    "dictionary.save('text.dict')\n",
    "\n",
    "## Term frequencyâ€“inverse document frequency (TF-IDF)\n",
    "## Method to reflect how important a word is to a document in a collection.\n",
    "## The inverse document frequency measures whether the term is common or rare across all documents.\n",
    "bow = [dictionary.doc2bow(l) for l in lemm] # Calculates inverse document counts for all terms\n",
    "tfidf = models.TfidfModel(bow)              # Transforms the count representation into the Tfidf space\n",
    "corpus_tfidf = tfidf[bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Build the LSI model\n",
    "## Method to uses a mathematical technique called singular value decomposition (SVD) \n",
    "## to identify patterns in the relationships between the terms and concepts contained \n",
    "## in an unstructured collection of text.\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.54099716727821068, u'better'), (-0.27853037146820386, u'complex'), (-0.27731795093733819, u'now'), (-0.25384405967038415, u'never'), (-0.18163236900752361, u'dense'), (-0.18163236900752347, u'nested'), (-0.18163236900752339, u'sparse'), (-0.18163236900752333, u'ugly'), (-0.1816323690075233, u'flat'), (-0.18163236900752308, u'implicit')]\n",
      "[(-0.30794775806820202, u'one'), (-0.25292223047307155, u'implementation'), (-0.25292223047307155, u'idea'), (-0.25292223047307155, u'explain'), (-0.2077880505088438, u'do'), (-0.20437009294542663, u'bad'), (-0.20437009294542663, u'hard'), (0.19505419477989211, u'better'), (-0.18973512044056631, u'may'), (-0.18226648505043938, u'obvious')]\n",
      "[(-0.24330841921959806, u'bad'), (-0.243308419219598, u'hard'), (-0.23373524267885931, u'explain'), (-0.23373524267885928, u'implementation'), (-0.23373524267885928, u'idea'), (0.20928347389567789, u'never'), (0.20727316973039564, u'although'), (0.17738986606596713, u'should'), (0.17342762797286618, u'obvious'), (0.17342762797286618, u'way')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(lsi.num_topics):\n",
    "    print lsi.show_topic(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'better', u'complex', u'now', u'never', u'dense', u'nested', u'sparse', u'ugly', u'flat', u'implicit', u'one', u'implementation', u'idea', u'explain', u'do', u'bad', u'hard', u'better', u'may', u'obvious', u'bad', u'hard', u'explain', u'implementation', u'idea', u'never', u'although', u'should', u'obvious', u'way']\n"
     ]
    }
   ],
   "source": [
    "list_topics = [] \n",
    "for i in range(lsi.num_topics):\n",
    "    list_topics.extend(lsi.show_topic(i))\n",
    "    \n",
    "topics = [i[1] for i in list_topics]\n",
    "print topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
